import json
import re
import numpy as np
import warnings
from typing import Dict, List, Set, Optional
import math
import time
import os
from pathlib import Path
from dotenv import load_dotenv
from datetime import datetime

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
import pymorphy2
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes, CallbackQueryHandler

warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
ADMIN_USER_ID = 1373472999
CONSULTATIONS_FILE = "consultations.json"
CALENDAR_URL = "https://calendar.app.google/ThpteAc5uqhxqnUA9"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
morph = pymorphy2.MorphAnalyzer()

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
RUSSIAN_STOPWORDS = {
    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ',
    '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞',
    '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç',
    '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '—É–∂–µ', '–≤—Å–µ–≥–æ', '–≤—Å—ë',
    '–±—ã—Ç—å', '–±—É–¥–µ—Ç', '—Å–∫–∞–∑–∞–ª', '—ç—Ç–æ—Ç', '—ç—Ç–æ', '–∑–¥–µ—Å—å', '—Ç–æ—Ç', '—Ç–∞–º', '–≥–¥–µ',
    '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä—ã–µ', '–∏—Ö', '—ç—Ç–æ–≥–æ', '—ç—Ç–æ–π', '—ç—Ç–æ–º—É', '—ç—Ç–∏–º',
    '—ç—Ç–∏', '—ç—Ç–∏—Ö', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–µ', '–≤–∞—à–µ–≥–æ', '–≤–∞—à–µ–π', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è',
    '–∫–∞–∫–æ–µ', '–∫–∞–∫–∏–µ', '–∫–∞–∫–æ–≥–æ', '–∫–∞–∫–æ–º', '–∫–∞–∫–∏–º–∏', '–º—ã', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ',
    '–º–æ–π', '–º–æ—è', '–º–æ—ë', '–º–æ–∏', '—Ç–≤–æ–π', '—Ç–≤–æ—è', '—Ç–≤–æ—ë', '—Ç–≤–æ–∏', '—Å–∞–º', '—Å–∞–º–∞',
    '—Å–∞–º–æ', '—Å–∞–º–∏', '—Ç–æ—Ç', '—Ç–∞', '—Ç–æ', '—Ç–µ', '—á–µ–π', '—á—å—è', '—á—å—ë', '—á—å–∏', '–∫—Ç–æ',
    '—á—Ç–æ', '–≥–¥–µ', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫–∞–∫', '–ª–∏–±–æ',
    '–Ω–∏–±—É–¥—å', '—Ç–∞–∫–∂–µ', '–ø–æ—Ç–æ–º—É', '—á—Ç–æ–±—ã', '–∫–æ—Ç–æ—Ä—ã–π', '—Å–≤–æ–π', '—Å–≤–æ—è', '—Å–≤–æ—ë',
    '—Å–≤–æ–∏', '—Å–∞–º—ã–π', '—Å–∞–º–∞—è', '—Å–∞–º–æ–µ', '—Å–∞–º—ã–µ', '–∏–ª–∏', '–Ω—É', '—ç—Ö', '–∞—Ö', '–æ—Ö',
    '–±–µ–∑', '–Ω–∞–¥', '–ø–æ–¥', '–ø–µ—Ä–µ–¥', '–ø–æ—Å–ª–µ', '–º–µ–∂–¥—É', '—á–µ—Ä–µ–∑', '—á—Ç–æ–±—ã', '—Ä–∞–¥–∏',
    '–¥–ª—è', '–¥–æ', '–ø–æ—Å–ª–µ', '–æ–∫–æ–ª–æ', '–≤–æ–∑–ª–µ', '—Ä—è–¥–æ–º', '–º–∏–º–æ', '–≤–æ–∫—Ä—É–≥', '–ø—Ä–æ—Ç–∏–≤',
    '–∑–∞', '–Ω–∞–¥–æ', '–Ω—É–∂–Ω–æ', '–º–æ–∂–µ—Ç', '–º–æ–∂–Ω–æ', '–¥–æ–ª–∂–µ–Ω', '–¥–æ–ª–∂–Ω–∞', '–¥–æ–ª–∂–Ω–æ',
    '–¥–æ–ª–∂–Ω—ã', '—Ö–æ—á—É', '—Ö–æ—á–µ—à—å', '—Ö–æ—á–µ—Ç', '—Ö–æ—Ç–∏–º', '—Ö–æ—Ç–∏—Ç–µ', '—Ö–æ—Ç—è—Ç', '–±—É–¥—É',
    '–±—É–¥–µ—à—å', '–±—É–¥–µ—Ç', '–±—É–¥–µ–º', '–±—É–¥–µ—Ç–µ', '–±—É–¥—É—Ç', '—Ö–æ—Ç—è', '–µ—Å–ª–∏', '–ø–æ–∫–∞',
    '—á—Ç–æ–±', '–∑–∞—Ç–æ', '–∏—Ç–∞–∫', '—Ç–∞–∫–∂–µ', '—Ç–æ–∂–µ'
}

# –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
SYNONYMS = {
    '—Å—Ç–æ–∏–º–æ—Å—Ç—å': ['—Ü–µ–Ω–∞', '—Ç–∞—Ä–∏—Ñ', '–ø–ª–∞—Ç–∞', '—Ä–∞—Å—Ü–µ–Ω–∫–∞'],
    '–∫—É—Ä—Å': ['–æ–±—É—á–µ–Ω–∏–µ', '–ø—Ä–æ–≥—Ä–∞–º–º–∞', '—Ç—Ä–µ–Ω–∏–Ω–≥'],
    '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å': ['—É—á–∏—Ç–µ–ª—å', '—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä', '—Ç—Ä–µ–Ω–µ—Ä', '–ª–µ–∫—Ç–æ—Ä'],
    '–∑–∞–Ω—è—Ç–∏–µ': ['—É—Ä–æ–∫', '–ª–µ–∫—Ü–∏—è', '–ø–∞—Ä–∞', '–≤—Å—Ç—Ä–µ—á–∞'],
    '–≥—Ä—É–ø–ø–∞': ['–∫–æ–º–∞–Ω–¥–∞', '–∫–æ–ª–ª–µ–∫—Ç–∏–≤'],
    '–º–µ—Ç–æ–¥': ['–ø–æ–¥—Ö–æ–¥', '—Ç–µ—Ö–Ω–∏–∫–∞', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è'],
    '–¥–æ–º–∞—à–∫–∞': ['–∑–∞–¥–∞–Ω–∏–µ', '–¥–∑', '–ø—Ä–∞–∫—Ç–∏–∫–∞'],
    '–±–æ—Ç': ['—á–∞—Ç-–±–æ—Ç', '–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç', '–ø–æ–º–æ—â–Ω–∏–∫'],
    'python': ['–ø–∏—Ç–æ–Ω', '–ø–∞–π—Ç–æ–Ω'],
    '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ': ['–∫–æ–¥–∏–Ω–≥', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞'],
    '–≤–æ–ø—Ä–æ—Å': ['–∑–∞–ø—Ä–æ—Å', '–ø—Ä–æ–±–ª–µ–º–∞', '—Ç–µ–º–∞'],
    '–æ—Ç–≤–µ—Ç': ['—Ä–µ—à–µ–Ω–∏–µ', '–æ—Ç–∫–ª–∏–∫'],
    '–Ω–∞—á–∞—Ç—å': ['—Å—Ç–∞—Ä—Ç–æ–≤–∞—Ç—å', '–ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å'],
    '–∑–∞–ø–∏—Å–∞—Ç—å—Å—è': ['–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è', '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è'],
    '—Å–ª–æ–∂–Ω—ã–π': ['—Ç—Ä—É–¥–Ω—ã–π', '–∑–∞–º—ã—Å–ª–æ–≤–∞—Ç—ã–π', '–∑–∞–ø—É—Ç–∞–Ω–Ω—ã–π'],
    '–ª–µ–≥–∫–∏–π': ['–ø—Ä–æ—Å—Ç–æ–π', '–Ω–µ—Ç—Ä—É–¥–Ω—ã–π'],
    '–±—ã—Å—Ç—Ä–æ': ['—Å–∫–æ—Ä–æ—Å—Ç—å', '–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ', '–≤ —Å—Ä–æ–∫'],
    '–¥–æ–ª–≥–æ': ['–º–µ–¥–ª–µ–Ω–Ω–æ', '–∑–∞—Ç—è–Ω—É—Ç–æ'],
    '–∫–∞—á–µ—Å—Ç–≤–æ': ['—É—Ä–æ–≤–µ–Ω—å', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç'],
    '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è': ['–≤—Å—Ç—Ä–µ—á–∞', '—Å–æ–≤–µ—Ç', '–ø–æ–º–æ—â—å'],
    '–¥–æ—Å—Ç—É–ø': ['–ø–æ–ª—É—á–µ–Ω–∏–µ', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å'],
    '–º–∞—Ç–µ—Ä–∏–∞–ª—ã': ['—É—Ä–æ–∫–∏', '–ª–µ–∫—Ü–∏–∏', '—Ä–µ—Å—É—Ä—Å—ã'],
    '–ø–æ–¥–¥–µ—Ä–∂–∫–∞': ['–ø–æ–º–æ—â—å', '—Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ']
}

def preprocess_question(question: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –≤–≤–æ–¥–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞"""
    patterns = [
        r'^–∞ –µ—Å–ª–∏\s+',
        r'^—á—Ç–æ –µ—Å–ª–∏\s+',
        r'^—á—Ç–æ –±—É–¥–µ—Ç –µ—Å–ª–∏\s+',
        r'^–º–æ–∂–Ω–æ –ª–∏\s+',
        r'^–∞ —á—Ç–æ –µ—Å–ª–∏\s+',
        r'^–µ—Å–ª–∏ —è\s+',
        r'^–∞\s+',
        r'^–Ω—É\s+',
        r'^—Å–∫–∞–∂–∏\s+',
        r'^—Ä–∞—Å—Å–∫–∞–∂–∏\s+',
        r'^–æ–±—ä—è—Å–Ω–∏\s+'
    ]
    
    cleaned = question.lower()
    for pattern in patterns:
        cleaned = re.sub(pattern, '', cleaned)
    
    return cleaned.strip()

def expand_with_synonyms(keywords: Set[str]) -> Set[str]:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
    expanded = set(keywords)
    for word in keywords:
        for base, synonyms in SYNONYMS.items():
            if word == base or any(word == syn for syn in synonyms):
                expanded.update([base] + synonyms)
    return expanded

def load_knowledge_base(file_path: str) -> list:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ JSON-—Ñ–∞–π–ª–∞"""
    path = Path(file_path)
    if not path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def preprocess_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    # –£–¥–∞–ª—è–µ–º URL, email –∏ —Ç.–¥.
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    # –û—Å–Ω–æ–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
    return re.sub(r'[^\w\s]', ' ', text.lower().strip())

def lemmatize_word(word: str) -> str:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    if not hasattr(lemmatize_word, 'cache'):
        lemmatize_word.cache = {}
    
    if word in lemmatize_word.cache:
        return lemmatize_word.cache[word]
    
    parsed = morph.parse(word)[0]
    lemma = parsed.normal_form
    lemmatize_word.cache[word] = lemma
    return lemma

def lemmatize_sentence(text: str) -> str:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
    # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –≤–æ–ø—Ä–æ—Å–∞ –∏ –¥—Ä—É–≥–∏–µ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'[?!.]', '', text)
    words = preprocess_text(text).split()
    lemmas = [lemmatize_word(word) for word in words if not is_stop_word(word) and len(word) > 2]
    return " ".join(lemmas)

def is_stop_word(word: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ–º"""
    return word.lower() in RUSSIAN_STOPWORDS

def extract_keywords(text: str, use_synonyms: bool = True) -> set:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π"""
    cleaned_text = preprocess_text(text)
    words = cleaned_text.split()
    
    keywords = {
        lemmatize_word(word) for word in words
        if len(word) > 2 and not is_stop_word(word)
    }
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    if use_synonyms:
        keywords = expand_with_synonyms(keywords)
    
    return keywords

def extract_entities(text: str) -> dict:
    """–ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (—á–∏—Å–ª–∞, –¥–∞—Ç—ã, –∏–º–µ–Ω–∞)"""
    entities = {
        'numbers': re.findall(r'\d+', text),
        'money': re.findall(r'\d+\s*(?:—Ä—É–±|—Ä|—Ä—É–±–ª–µ–π|–¥–æ–ª–ª–∞—Ä–æ–≤|usd|eur)', text, re.IGNORECASE),
        'timeframes': re.findall(r'\d+\s*(?:—á–∞—Å|–º–∏–Ω—É—Ç|–¥–Ω–µ–π|–Ω–µ–¥–µ–ª|–º–µ—Å—è—Ü|–≥–æ–¥)', text, re.IGNORECASE)
    }
    return entities

def calculate_keyword_match_score(user_keywords: Set[str], item_keywords: Set[str], 
                                 user_question: str, original_keywords: List[str]) -> float:
    """–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º"""
    # –ë–∞–∑–æ–≤–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ª–µ–º–º
    common_keywords = user_keywords.intersection(item_keywords)
    base_score = len(common_keywords) * 2
    
    # –ë–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ñ—Ä–∞–∑
    question_lower = preprocess_text(user_question)
    phrase_bonus = 0
    
    for orig_keyword in original_keywords:
        keyword_lower = preprocess_text(orig_keyword)
        if keyword_lower in question_lower:
            # –ë–æ–Ω—É—Å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã —Ñ—Ä–∞–∑—ã –∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            phrase_bonus += len(keyword_lower.split()) * 3
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    context_bonus = 0
    question_lemmas = set(preprocess_text(user_question).split())
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —á–∏—Å–ª–∞ –≤ –≤–æ–ø—Ä–æ—Å–µ –∏ –≤ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö
    question_numbers = set(re.findall(r'\d+', user_question))
    keyword_numbers = set()
    for kw in original_keywords:
        keyword_numbers.update(re.findall(r'\d+', kw))
    
    if question_numbers and keyword_numbers and question_numbers.intersection(keyword_numbers):
        context_bonus += 5
    
    # –°—É–º–º–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    total_score = base_score + phrase_bonus + context_bonus
    
    return total_score

class KBIndex:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    def __init__(self):
        self.items = []
        self.contexts = []
        self.tfidf_vectorizer = None
        self.tfidf_labeled_matrix = None
        self.raw_tfidf_vectorizer = None
        self.tfidf_raw_matrix = None
        self.last_update = 0
    
    def build_tfidf_index(self, contexts: List[str]):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ TF-IDF –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        # –î–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        self.tfidf_vectorizer = TfidfVectorizer(
            lowercase=True,
            stop_words=list(RUSSIAN_STOPWORDS),
            ngram_range=(1, 3),
            max_features=3000  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        )
        
        lemmatized_contexts = [lemmatize_sentence(ctx) for ctx in contexts]
        self.tfidf_labeled_matrix = self.tfidf_vectorizer.fit_transform(lemmatized_contexts)
        
        # –î–ª—è –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞)
        self.raw_tfidf_vectorizer = TfidfVectorizer(
            lowercase=True,
            stop_words=list(RUSSIAN_STOPWORDS),
            ngram_range=(1, 2),
            max_features=2000
        )
        self.tfidf_raw_matrix = self.raw_tfidf_vectorizer.fit_transform(contexts)
    
    def keyword_search(self, user_question: str, top_k: int = 3) -> List[dict]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        user_keywords = extract_keywords(user_question)
        print(f"üîë –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ '{user_question}': {user_keywords}")
        
        if not user_keywords:
            return []
        
        scored_items = []
        for idx, item in enumerate(self.items):
            score = calculate_keyword_match_score(
                user_keywords, 
                item["keywords"], 
                user_question,
                item["original_keywords"]
            )
            
            if score > 0:
                scored_items.append({
                    "context": item["context"],
                    "score": score,
                    "index": idx
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ü–µ–Ω–∫–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K
        scored_items.sort(key=lambda x: x["score"], reverse=True)
        return scored_items[:top_k]
    
    def fulltext_search(self, query: str, top_k: int = 3) -> List[dict]:
        """–ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF"""
        if self.tfidf_vectorizer is None or self.tfidf_labeled_matrix is None:
            return []
        
        results = []
        
        try:
            # –ü–æ–∏—Å–∫ –ø–æ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
            query_lemma = lemmatize_sentence(query)
            query_vec = self.tfidf_vectorizer.transform([query_lemma])
            labeled_similarities = cosine_similarity(query_vec, self.tfidf_labeled_matrix)[0]
            
            # –ü–æ–∏—Å–∫ –ø–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
            raw_query_vec = self.raw_tfidf_vectorizer.transform([query])
            raw_similarities = cosine_similarity(raw_query_vec, self.tfidf_raw_matrix)[0]
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            combined_similarities = 0.7 * labeled_similarities + 0.3 * raw_similarities
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            top_indices = np.argsort(combined_similarities)[::-1][:top_k]
            
            for idx in top_indices:
                score = combined_similarities[idx]
                if score > 0.15:  # –ü–æ—Ä–æ–≥ –¥–ª—è TF-IDF
                    results.append({
                        "context": self.contexts[idx],
                        "score": float(score),
                        "index": int(idx)
                    })
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–º –ø–æ–∏—Å–∫–µ: {str(e)}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        
        return results

def preprocess_knowledge_base(knowledge_base: list) -> KBIndex:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–ª–∞—Å—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    kb_index = KBIndex()
    processed_items = []
    
    contexts = [item["context"] for item in knowledge_base]
    
    for i, item in enumerate(knowledge_base):
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –±–∞–∑—ã
        processed_keywords = set()
        for keyword in item["keywords"]:
            for word in re.split(r'\s+', preprocess_text(keyword)):
                if len(word) > 2 and not is_stop_word(word):
                    processed_keywords.add(lemmatize_word(word))
        
        item_data = {
            "context": item["context"],
            "keywords": processed_keywords,
            "original_keywords": item["keywords"]
        }
        
        processed_items.append(item_data)
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
    kb_index.items = processed_items
    kb_index.contexts = contexts
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ TF-IDF –∏–Ω–¥–µ–∫—Å–∞
    kb_index.build_tfidf_index(contexts)
    
    kb_index.last_update = time.time()
    return kb_index

def find_best_match(user_question: str, kb_index: KBIndex) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –≤–≤–æ–¥–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
    cleaned_question = preprocess_question(user_question)
    print(f"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å: '{user_question}'")
    print(f"–û—á–∏—â–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å: '{cleaned_question}'")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
    entities = extract_entities(user_question)
    
    # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –¥–ª—è –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
    keyword_results = kb_index.keyword_search(cleaned_question, top_k=5)
    
    # –ü–æ–∏—Å–∫ –ø–æ –ø–æ–ª–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –¥–ª—è –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
    fulltext_results = kb_index.fulltext_search(cleaned_question, top_k=5)
    
    # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ - –ø—Ä–æ–±—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å
    if not keyword_results and not fulltext_results:
        print("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞, –ø—Ä–æ–±—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å")
        keyword_results = kb_index.keyword_search(user_question, top_k=5)
        fulltext_results = kb_index.fulltext_search(user_question, top_k=5)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤–µ—Å–∞–º–∏
    combined_results = {}
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–≤–µ—Å 0.6)
    for res in keyword_results:
        idx = res["index"]
        combined_results.setdefault(idx, 0)
        combined_results[idx] += res["score"] * 0.6
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–≤–µ—Å 0.4)
    for res in fulltext_results:
        idx = res["index"]
        combined_results.setdefault(idx, 0)
        combined_results[idx] += res["score"] * 50 * 0.4
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–µ
    if combined_results:
        sorted_results = sorted(combined_results.items(), key=lambda x: x[1], reverse=True)
        best_idx, best_score = sorted_results[0]
        
        # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è –æ—Ç–≤–µ—Ç–∞
        if best_score > 1.5:  # –ë—ã–ª–æ 3.0
            print(f"–ù–∞–π–¥–µ–Ω –æ—Ç–≤–µ—Ç —Å –æ—Ü–µ–Ω–∫–æ–π {best_score} –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ '{cleaned_question}'")
            return kb_index.items[best_idx]["context"]
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ —Ö–æ—Ä–æ—à–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
    if fulltext_results:
        best_fulltext = fulltext_results[0]
        if best_fulltext["score"] > 0.2:  # –ë—ã–ª–æ 0.3
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—Ü–µ–Ω–∫–æ–π {best_fulltext['score']}")
            return best_fulltext["context"]
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - –ø—Ä–æ–±—É–µ–º –ø–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    fallback_keywords = extract_keywords(cleaned_question, use_synonyms=False)
    if fallback_keywords:
        print(f"–ü–æ–ø—ã—Ç–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {fallback_keywords}")
        fallback_results = kb_index.keyword_search(" ".join(fallback_keywords), top_k=3)
        if fallback_results and fallback_results[0]["score"] > 0:
            return fallback_results[0]["context"]
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
    return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –≤ —Å–≤–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –¥—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –∏–ª–∏ —É—Ç–æ—á–Ω–∏—Ç—å –¥–µ—Ç–∞–ª–∏."

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
kb_index = None
user_contexts = {}

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    welcome_message = (
        "–ü—Ä–∏–≤–µ—Ç! –Ø –≤–∞—à —É—á–µ–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. "
        "–ó–∞–¥–∞–π—Ç–µ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å –ø–æ –∫—É—Ä—Å—É, –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç.\n\n"
        "‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∞–Ω–∞–ª–∏–∑—É."
    )
    await update.message.reply_text(welcome_message)

async def consultation_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏ –∑–∞–ø–∏—Å–∏ –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é"""
    query = update.callback_query
    await query.answer()
    
    user = query.from_user
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_data = {
        "user_id": user.id,
        "username": user.username or "–ù–µ—Ç username",
        "first_name": user.first_name or "",
        "last_name": user.last_name or "",
        "timestamp": timestamp
    }
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫
    consultations = []
    if os.path.exists(CONSULTATIONS_FILE):
        with open(CONSULTATIONS_FILE, "r", encoding="utf-8") as f:
            try:
                consultations = json.load(f)
            except json.JSONDecodeError:
                consultations = []
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
    consultations.append(user_data)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    with open(CONSULTATIONS_FILE, "w", encoding="utf-8") as f:
        json.dump(consultations, f, ensure_ascii=False, indent=4)
    
    # –°–æ–∑–¥–∞–µ–º –∫–Ω–æ–ø–∫–∏ —Å —Å—Å—ã–ª–∫–æ–π –Ω–∞ –∫–∞–ª–µ–Ω–¥–∞—Ä—å
    keyboard = [
        [InlineKeyboardButton("üìÖ –ü–µ—Ä–µ–π—Ç–∏ –∫ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é", url=CALENDAR_URL)],
        [InlineKeyboardButton("üì± –ù–∞–ø–∏—Å–∞—Ç—å –≤ Telegram", url="https://t.me/AVick23")]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ —Å–≤—è–∑–∏
    await query.edit_message_text(
        text="‚úÖ –í–∞—à–∞ –∑–∞—è–≤–∫–∞ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!\n\n"
             "–í—ã –º–æ–∂–µ—Ç–µ:\n"
             "1. üîó <b>–í—ã–±—Ä–∞—Ç—å —É–¥–æ–±–Ω–æ–µ –≤—Ä–µ–º—è —Å–∞–º–∏</b> —á–µ—Ä–µ–∑ Google –ö–∞–ª–µ–Ω–¥–∞—Ä—å\n"
             "2. üì± <b>–ù–∞–ø–∏—Å–∞—Ç—å –º–Ω–µ –Ω–∞–ø—Ä—è–º—É—é</b> –≤ Telegram –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏\n\n"
             "–Ø —Ç–∞–∫–∂–µ —Å–≤—è–∂—É—Å—å —Å –≤–∞–º–∏ –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π.",
        reply_markup=reply_markup,
        parse_mode="HTML"
    )

async def clear_list_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏ –æ—á–∏—Å—Ç–∫–∏ —Å–ø–∏—Å–∫–∞ –∑–∞—è–≤–æ–∫"""
    query = update.callback_query
    await query.answer()
    
    # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª –∑–∞—è–≤–æ–∫
    with open(CONSULTATIONS_FILE, "w", encoding="utf-8") as f:
        json.dump([], f, ensure_ascii=False, indent=4)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    await query.edit_message_text(text="‚úÖ –°–ø–∏—Å–æ–∫ –∑–∞—è–≤–æ–∫ —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω!")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    user_id = update.effective_user.id
    user_question = update.message.text.strip().lower()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã "–∑–∞—è–≤–∫–∏" —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞
    if user_id == ADMIN_USER_ID and user_question == "–∑–∞—è–≤–∫–∏":
        if not os.path.exists(CONSULTATIONS_FILE) or os.path.getsize(CONSULTATIONS_FILE) == 0:
            await update.message.reply_text("üìã –°–ø–∏—Å–æ–∫ –∑–∞—è–≤–æ–∫ –ø—É—Å—Ç.")
            return
        
        try:
            with open(CONSULTATIONS_FILE, "r", encoding="utf-8") as f:
                consultations = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            consultations = []
        
        if not consultations:
            await update.message.reply_text("üìã –°–ø–∏—Å–æ–∫ –∑–∞—è–≤–æ–∫ –ø—É—Å—Ç.")
            return
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–æ —Å–ø–∏—Å–∫–æ–º –∑–∞—è–≤–æ–∫
        message = "üìã –°–ø–∏—Å–æ–∫ –∑–∞—è–≤–æ–∫:\n\n"
        for idx, consult in enumerate(consultations, 1):
            username = consult.get('username', '–ù–µ—Ç username')
            first_name = consult.get('first_name', '')
            last_name = consult.get('last_name', '')
            timestamp = consult.get('timestamp', '')
            
            message += f"{idx}. {first_name} {last_name}\n"
            message += f"   üë§ @{username}\n"
            message += f"   ‚è∞ {timestamp}\n\n"
        
        # –°–æ–∑–¥–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—á–∏—Å—Ç–∫–∏ —Å–ø–∏—Å–∫–∞
        keyboard = [[InlineKeyboardButton("–û—á–∏—Å—Ç–∏—Ç—å —Å–ø–∏—Å–æ–∫", callback_data="clear_list")]]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        await update.message.reply_text(message, reply_markup=reply_markup)
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if user_id not in user_contexts:
        user_contexts[user_id] = {"last_answer": None}
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤
    short_answers = ['–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–∞–≥–∞', '—É–≥—É', '–µ—â–µ', '–±–æ–ª—å—à–µ', '—Ä–∞—Å—Å–∫–∞–∂–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ', '–∫–∞–∫?', '–ø–æ—á–µ–º—É?']
    if user_question in short_answers:
        last_answer = user_contexts[user_id]["last_answer"]
        if last_answer:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –∫–Ω–æ–ø–∫—É –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏
            if "[add_button]" in last_answer:
                clean_answer = last_answer.replace("[add_button]", "").strip()
                keyboard = [[InlineKeyboardButton("–ó–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é", callback_data="consultation")]]
                reply_markup = InlineKeyboardMarkup(keyboard)
                await update.message.reply_text(clean_answer, reply_markup=reply_markup)
            else:
                await update.message.reply_text(last_answer)
            return
    
    # –ü–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞
    answer = find_best_match(update.message.text, kb_index)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ—á–∏—â–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –±–µ–∑ –º–∞—Ä–∫–µ—Ä–∞)
    clean_answer = answer.replace("[add_button]", "").strip()
    user_contexts[user_id]["last_answer"] = clean_answer
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –∫–Ω–æ–ø–∫—É –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏
    if "[add_button]" in answer:
        keyboard = [
            [InlineKeyboardButton("üìÖ –ó–∞–ø–∏—Å–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ Google –ö–∞–ª–µ–Ω–¥–∞—Ä—å", url=CALENDAR_URL)],
            [InlineKeyboardButton("üìù –û—Å—Ç–∞–≤–∏—Ç—å –∑–∞—è–≤–∫—É –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏", callback_data="consultation")]
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)
        await update.message.reply_text(
            clean_answer + "\n\nüí° –í—ã –º–æ–∂–µ—Ç–µ –≤—ã–±—Ä–∞—Ç—å —É–¥–æ–±–Ω–æ–µ –≤—Ä–µ–º—è —Å–∞–º–∏ —á–µ—Ä–µ–∑ –∫–∞–ª–µ–Ω–¥–∞—Ä—å –∏–ª–∏ –æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞—è–≤–∫—É –∏ —è —Å–≤—è–∂—É—Å—å —Å –≤–∞–º–∏ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏.",
            reply_markup=reply_markup,
            parse_mode="HTML"
        )
    else:
        await update.message.reply_text(clean_answer)

async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫"""
    print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {context.error}")
    if update and hasattr(update, 'message'):
        await update.message.reply_text(
            "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. "
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –µ–≥–æ."
        )

def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞"""
    global kb_index
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞ –∏–∑ .env —Ñ–∞–π–ª–∞
    token = os.getenv("BOT_TOKEN")
    if not token:
        raise ValueError("–¢–æ–∫–µ–Ω –±–æ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ. –£–∫–∞–∂–∏—Ç–µ BOT_TOKEN=–≤–∞—à_—Ç–æ–∫–µ–Ω")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    try:
        kb = load_knowledge_base('main.json')
        kb_index = preprocess_knowledge_base(kb)
        print("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {str(e)}")
        raise
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    application = Application.builder().token(token).build()
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    application.add_handler(CallbackQueryHandler(consultation_callback, pattern="consultation"))
    application.add_handler(CallbackQueryHandler(clear_list_callback, pattern="clear_list"))
    application.add_error_handler(error_handler)
    
    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    application.run_polling()

if __name__ == "__main__":
    main()