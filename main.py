import json
import re
import numpy as np
import warnings
from typing import Dict, List, Set, Optional
import math
import time
import os
from pathlib import Path
from dotenv import load_dotenv
from datetime import datetime

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
import pymorphy2
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes, CallbackQueryHandler

warnings.filterwarnings('ignore', category=UserWarning, module='transformers')

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
ADMIN_USER_ID = 1373472999
CONSULTATIONS_FILE = "consultations.json"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
morph = pymorphy2.MorphAnalyzer()

# –ó–∞–≥—Ä—É–∑–∫–∞ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
try:
    # –ú–æ–¥–µ–ª—å —Å —Ö–æ—Ä–æ—à–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device='cpu')
    SEMANTIC_SEARCH_AVAILABLE = True
except Exception as e:
    print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º. –û—à–∏–±–∫–∞: {str(e)}")
    SEMANTIC_SEARCH_AVAILABLE = False

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
RUSSIAN_STOPWORDS = {
    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ',
    '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞',
    '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç',
    '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '—É–∂–µ', '–≤—Å–µ–≥–æ', '–≤—Å—ë',
    '–±—ã—Ç—å', '–±—É–¥–µ—Ç', '—Å–∫–∞–∑–∞–ª', '—ç—Ç–æ—Ç', '—ç—Ç–æ', '–∑–¥–µ—Å—å', '—Ç–æ—Ç', '—Ç–∞–º', '–≥–¥–µ',
    '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä—ã–µ', '–∏—Ö', '—ç—Ç–æ–≥–æ', '—ç—Ç–æ–π', '—ç—Ç–æ–º—É', '—ç—Ç–∏–º',
    '—ç—Ç–∏', '—ç—Ç–∏—Ö', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–µ', '–≤–∞—à–µ–≥–æ', '–≤–∞—à–µ–π', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è',
    '–∫–∞–∫–æ–µ', '–∫–∞–∫–∏–µ', '–∫–∞–∫–æ–≥–æ', '–∫–∞–∫–æ–º', '–∫–∞–∫–∏–º–∏', '–º—ã', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ',
    '–º–æ–π', '–º–æ—è', '–º–æ—ë', '–º–æ–∏', '—Ç–≤–æ–π', '—Ç–≤–æ—è', '—Ç–≤–æ—ë', '—Ç–≤–æ–∏', '—Å–∞–º', '—Å–∞–º–∞',
    '—Å–∞–º–æ', '—Å–∞–º–∏', '—Ç–æ—Ç', '—Ç–∞', '—Ç–æ', '—Ç–µ', '—á–µ–π', '—á—å—è', '—á—å—ë', '—á—å–∏', '–∫—Ç–æ',
    '—á—Ç–æ', '–≥–¥–µ', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫–∞–∫', '–ª–∏–±–æ',
    '–Ω–∏–±—É–¥—å', '—Ç–∞–∫–∂–µ', '–ø–æ—Ç–æ–º—É', '—á—Ç–æ–±—ã', '–∫–æ—Ç–æ—Ä—ã–π', '—Å–≤–æ–π', '—Å–≤–æ—è', '—Å–≤–æ—ë',
    '—Å–≤–æ–∏', '—Å–∞–º—ã–π', '—Å–∞–º–∞—è', '—Å–∞–º–æ–µ', '—Å–∞–º—ã–µ', '–∏–ª–∏', '–Ω—É', '—ç—Ö', '–∞—Ö', '–æ—Ö',
    '–±–µ–∑', '–Ω–∞–¥', '–ø–æ–¥', '–ø–µ—Ä–µ–¥', '–ø–æ—Å–ª–µ', '–º–µ–∂–¥—É', '—á–µ—Ä–µ–∑', '—á—Ç–æ–±—ã', '—Ä–∞–¥–∏',
    '–¥–ª—è', '–¥–æ', '–ø–æ—Å–ª–µ', '–æ–∫–æ–ª–æ', '–≤–æ–∑–ª–µ', '—Ä—è–¥–æ–º', '–º–∏–º–æ', '–≤–æ–∫—Ä—É–≥', '–ø—Ä–æ—Ç–∏–≤',
    '–∑–∞', '–Ω–∞–¥–æ', '–Ω—É–∂–Ω–æ', '–º–æ–∂–µ—Ç', '–º–æ–∂–Ω–æ', '–¥–æ–ª–∂–µ–Ω', '–¥–æ–ª–∂–Ω–∞', '–¥–æ–ª–∂–Ω–æ',
    '–¥–æ–ª–∂–Ω—ã', '—Ö–æ—á—É', '—Ö–æ—á–µ—à—å', '—Ö–æ—á–µ—Ç', '—Ö–æ—Ç–∏–º', '—Ö–æ—Ç–∏—Ç–µ', '—Ö–æ—Ç—è—Ç', '–±—É–¥—É',
    '–±—É–¥–µ—à—å', '–±—É–¥–µ—Ç', '–±—É–¥–µ–º', '–±—É–¥–µ—Ç–µ', '–±—É–¥—É—Ç', '—Ö–æ—Ç—è', '–µ—Å–ª–∏', '–ø–æ–∫–∞',
    '—á—Ç–æ–±', '–∑–∞—Ç–æ', '–∏—Ç–∞–∫', '—Ç–∞–∫–∂–µ', '—Ç–æ–∂–µ'
}

# –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
SYNONYMS = {
    '—Å—Ç–æ–∏–º–æ—Å—Ç—å': ['—Ü–µ–Ω–∞', '—Ç–∞—Ä–∏—Ñ', '–ø–ª–∞—Ç–∞', '—Ä–∞—Å—Ü–µ–Ω–∫–∞'],
    '–∫—É—Ä—Å': ['–æ–±—É—á–µ–Ω–∏–µ', '–ø—Ä–æ–≥—Ä–∞–º–º–∞', '—Ç—Ä–µ–Ω–∏–Ω–≥'],
    '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å': ['—É—á–∏—Ç–µ–ª—å', '—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä', '—Ç—Ä–µ–Ω–µ—Ä', '–ª–µ–∫—Ç–æ—Ä'],
    '–∑–∞–Ω—è—Ç–∏–µ': ['—É—Ä–æ–∫', '–ª–µ–∫—Ü–∏—è', '–ø–∞—Ä–∞', '–≤—Å—Ç—Ä–µ—á–∞'],
    '–≥—Ä—É–ø–ø–∞': ['–∫–æ–º–∞–Ω–¥–∞', '–∫–æ–ª–ª–µ–∫—Ç–∏–≤'],
    '–º–µ—Ç–æ–¥': ['–ø–æ–¥—Ö–æ–¥', '—Ç–µ—Ö–Ω–∏–∫–∞', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è'],
    '–¥–æ–º–∞—à–∫–∞': ['–∑–∞–¥–∞–Ω–∏–µ', '–¥–∑', '–ø—Ä–∞–∫—Ç–∏–∫–∞'],
    '–±–æ—Ç': ['—á–∞—Ç-–±–æ—Ç', '–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç', '–ø–æ–º–æ—â–Ω–∏–∫'],
    'python': ['–ø–∏—Ç–æ–Ω', '–ø–∞–π—Ç–æ–Ω'],
    '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ': ['–∫–æ–¥–∏–Ω–≥', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞'],
    '–≤–æ–ø—Ä–æ—Å': ['–∑–∞–ø—Ä–æ—Å', '–ø—Ä–æ–±–ª–µ–º–∞', '—Ç–µ–º–∞'],
    '–æ—Ç–≤–µ—Ç': ['—Ä–µ—à–µ–Ω–∏–µ', '–æ—Ç–∫–ª–∏–∫'],
    '–Ω–∞—á–∞—Ç—å': ['—Å—Ç–∞—Ä—Ç–æ–≤–∞—Ç—å', '–ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å'],
    '–∑–∞–ø–∏—Å–∞—Ç—å—Å—è': ['–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è', '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è'],
    '—Å–ª–æ–∂–Ω—ã–π': ['—Ç—Ä—É–¥–Ω—ã–π', '–∑–∞–º—ã—Å–ª–æ–≤–∞—Ç—ã–π', '–∑–∞–ø—É—Ç–∞–Ω–Ω—ã–π'],
    '–ª–µ–≥–∫–∏–π': ['–ø—Ä–æ—Å—Ç–æ–π', '–Ω–µ—Ç—Ä—É–¥–Ω—ã–π'],
    '–±—ã—Å—Ç—Ä–æ': ['—Å–∫–æ—Ä–æ—Å—Ç—å', '–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ', '–≤ —Å—Ä–æ–∫'],
    '–¥–æ–ª–≥–æ': ['–º–µ–¥–ª–µ–Ω–Ω–æ', '–∑–∞—Ç—è–Ω—É—Ç–æ'],
    '–∫–∞—á–µ—Å—Ç–≤–æ': ['—É—Ä–æ–≤–µ–Ω—å', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç']
}

def expand_with_synonyms(keywords: Set[str]) -> Set[str]:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
    expanded = set(keywords)
    for word in keywords:
        for base, synonyms in SYNONYMS.items():
            if word == base or any(word == syn for syn in synonyms):
                expanded.update([base] + synonyms)
    return expanded

def load_knowledge_base(file_path: str) -> list:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ JSON-—Ñ–∞–π–ª–∞"""
    path = Path(file_path)
    if not path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def preprocess_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    # –£–¥–∞–ª—è–µ–º URL, email –∏ —Ç.–¥.
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    # –û—Å–Ω–æ–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
    return re.sub(r'[^\w\s]', ' ', text.lower().strip())

def lemmatize_word(word: str) -> str:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    if not hasattr(lemmatize_word, 'cache'):
        lemmatize_word.cache = {}
    
    if word in lemmatize_word.cache:
        return lemmatize_word.cache[word]
    
    parsed = morph.parse(word)[0]
    lemma = parsed.normal_form
    lemmatize_word.cache[word] = lemma
    return lemma

def lemmatize_sentence(text: str) -> str:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
    words = preprocess_text(text).split()
    lemmas = [lemmatize_word(word) for word in words if not is_stop_word(word) and len(word) > 2]
    return " ".join(lemmas)

def is_stop_word(word: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ–º"""
    return word.lower() in RUSSIAN_STOPWORDS

def extract_keywords(text: str, use_synonyms: bool = True) -> set:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π"""
    cleaned_text = preprocess_text(text)
    words = cleaned_text.split()
    
    keywords = {
        lemmatize_word(word) for word in words
        if len(word) > 2 and not is_stop_word(word)
    }
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    if use_synonyms:
        keywords = expand_with_synonyms(keywords)
    
    return keywords

def extract_entities(text: str) -> dict:
    """–ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (—á–∏—Å–ª–∞, –¥–∞—Ç—ã, –∏–º–µ–Ω–∞)"""
    entities = {
        'numbers': re.findall(r'\d+', text),
        'money': re.findall(r'\d+\s*(?:—Ä—É–±|—Ä|—Ä—É–±–ª–µ–π|–¥–æ–ª–ª–∞—Ä–æ–≤|usd|eur)', text, re.IGNORECASE),
        'timeframes': re.findall(r'\d+\s*(?:—á–∞—Å|–º–∏–Ω—É—Ç|–¥–Ω–µ–π|–Ω–µ–¥–µ–ª|–º–µ—Å—è—Ü|–≥–æ–¥)', text, re.IGNORECASE)
    }
    return entities

def calculate_keyword_match_score(user_keywords: Set[str], item_keywords: Set[str], 
                                 user_question: str, original_keywords: List[str]) -> float:
    """–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º"""
    # –ë–∞–∑–æ–≤–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ª–µ–º–º
    common_keywords = user_keywords.intersection(item_keywords)
    base_score = len(common_keywords) * 2
    
    # –ë–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ñ—Ä–∞–∑
    question_lower = preprocess_text(user_question)
    phrase_bonus = 0
    
    for orig_keyword in original_keywords:
        keyword_lower = preprocess_text(orig_keyword)
        if keyword_lower in question_lower:
            # –ë–æ–Ω—É—Å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã —Ñ—Ä–∞–∑—ã –∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            phrase_bonus += len(keyword_lower.split()) * 3
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    context_bonus = 0
    question_lemmas = set(preprocess_text(user_question).split())
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —á–∏—Å–ª–∞ –≤ –≤–æ–ø—Ä–æ—Å–µ –∏ –≤ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö
    question_numbers = set(re.findall(r'\d+', user_question))
    keyword_numbers = set()
    for kw in original_keywords:
        keyword_numbers.update(re.findall(r'\d+', kw))
    
    if question_numbers and keyword_numbers and question_numbers.intersection(keyword_numbers):
        context_bonus += 5
    
    # –°—É–º–º–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    total_score = base_score + phrase_bonus + context_bonus
    
    return total_score

class KBIndex:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    def __init__(self):
        self.items = []
        self.all_embeddings = None
        self.contexts = []
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.last_update = 0
    
    def build_tfidf_index(self, contexts: List[str]):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ TF-IDF –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        self.tfidf_vectorizer = TfidfVectorizer(
            lowercase=True,
            stop_words=list(RUSSIAN_STOPWORDS),
            ngram_range=(1, 3),
            max_features=5000
        )
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform([lemmatize_sentence(ctx) for ctx in contexts])
    
    def update_embeddings(self, embeddings: np.ndarray, contexts: List[str]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        self.all_embeddings = embeddings
        self.contexts = contexts
    
    def semantic_search(self, query: str, top_k: int = 3) -> List[dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º"""
        if self.all_embeddings is None or not SEMANTIC_SEARCH_AVAILABLE:
            return []
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = model.encode([query])[0].reshape(1, -1)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = cosine_similarity(query_embedding, self.all_embeddings)[0]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_indices = np.argsort(similarities)[::-1][:top_k]
        results = []
        
        for idx in top_indices:
            score = similarities[idx]
            if score > 0.3:  # –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                results.append({
                    "context": self.contexts[idx],
                    "score": float(score),
                    "index": int(idx)
                })
        
        return results
    
    def keyword_search(self, user_question: str, top_k: int = 3) -> List[dict]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        user_keywords = extract_keywords(user_question)
        if not user_keywords:
            return []
        
        scored_items = []
        for idx, item in enumerate(self.items):
            score = calculate_keyword_match_score(
                user_keywords, 
                item["keywords"], 
                user_question,
                item["original_keywords"]
            )
            
            if score > 0:
                scored_items.append({
                    "context": item["context"],
                    "score": score,
                    "index": idx
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ü–µ–Ω–∫–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K
        scored_items.sort(key=lambda x: x["score"], reverse=True)
        return scored_items[:top_k]
    
    def fulltext_search(self, query: str, top_k: int = 3) -> List[dict]:
        """–ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF"""
        if self.tfidf_vectorizer is None or self.tfidf_matrix is None:
            return []
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_lemma = lemmatize_sentence(query)
        query_vec = self.tfidf_vectorizer.transform([query_lemma])
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = cosine_similarity(query_vec, self.tfidf_matrix)[0]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_indices = np.argsort(similarities)[::-1][:top_k]
        results = []
        
        for idx in top_indices:
            score = similarities[idx]
            if score > 0.1:  # –ü–æ—Ä–æ–≥ –¥–ª—è TF-IDF
                results.append({
                    "context": self.contexts[idx],
                    "score": float(score),
                    "index": int(idx)
                })
        
        return results

def preprocess_knowledge_base(knowledge_base: list) -> KBIndex:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–ª–∞—Å—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    kb_index = KBIndex()
    processed_items = []
    
    contexts = [item["context"] for item in knowledge_base]
    
    for i, item in enumerate(knowledge_base):
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –±–∞–∑—ã
        processed_keywords = set()
        for keyword in item["keywords"]:
            for word in re.split(r'\s+', preprocess_text(keyword)):
                if len(word) > 2 and not is_stop_word(word):
                    processed_keywords.add(lemmatize_word(word))
        
        item_data = {
            "context": item["context"],
            "keywords": processed_keywords,
            "original_keywords": item["keywords"]
        }
        
        processed_items.append(item_data)
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
    kb_index.items = processed_items
    kb_index.contexts = contexts
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ TF-IDF –∏–Ω–¥–µ–∫—Å–∞
    kb_index.build_tfidf_index(contexts)
    
    # –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –≤—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    if SEMANTIC_SEARCH_AVAILABLE:
        embeddings = []
        batch_size = 8  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤...")
        for i in range(0, len(contexts), batch_size):
            batch = contexts[i:i+batch_size]
            batch_embeddings = model.encode(batch, show_progress_bar=False)
            embeddings.extend(batch_embeddings)
        
        kb_index.update_embeddings(np.array(embeddings), contexts)
    
    kb_index.last_update = time.time()
    return kb_index

def find_best_match(user_question: str, kb_index: KBIndex) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
    entities = extract_entities(user_question)
    
    # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    keyword_results = kb_index.keyword_search(user_question, top_k=3)
    
    # –ü–æ–∏—Å–∫ –ø–æ –ø–æ–ª–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
    fulltext_results = kb_index.fulltext_search(user_question, top_k=3)
    
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    semantic_results = kb_index.semantic_search(user_question, top_k=3)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤–µ—Å–∞–º–∏
    combined_results = {}
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–≤–µ—Å 0.5)
    for res in keyword_results:
        idx = res["index"]
        combined_results.setdefault(idx, 0)
        combined_results[idx] += res["score"] * 0.5
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–≤–µ—Å 0.3)
    for res in fulltext_results:
        idx = res["index"]
        combined_results.setdefault(idx, 0)
        combined_results[idx] += res["score"] * 50 * 0.3  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ü–µ–Ω–∫—É TF-IDF
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–≤–µ—Å 0.7)
    for res in semantic_results:
        idx = res["index"]
        combined_results.setdefault(idx, 0)
        combined_results[idx] += res["score"] * 10 * 0.7  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ü–µ–Ω–∫—É
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–µ
    if combined_results:
        best_idx = max(combined_results.items(), key=lambda x: x[1])[0]
        best_score = combined_results[best_idx]
        
        # –ï—Å–ª–∏ –æ—Ü–µ–Ω–∫–∞ –≤—ã—Å–æ–∫–∞—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if best_score > 5.0:
            return kb_index.items[best_idx]["context"]
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ —Ö–æ—Ä–æ—à–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    if semantic_results and semantic_results[0]["score"] > 0.5:
        return semantic_results[0]["context"]
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
    return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –≤ —Å–≤–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –¥—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –∏–ª–∏ —É—Ç–æ—á–Ω–∏—Ç—å –¥–µ—Ç–∞–ª–∏."

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
kb_index = None
user_contexts = {}

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    welcome_message = (
        "–ü—Ä–∏–≤–µ—Ç! –Ø –≤–∞—à —É—á–µ–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. "
        "–ó–∞–¥–∞–π—Ç–µ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å –ø–æ –∫—É—Ä—Å—É, –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç.\n\n"
        f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: {'‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω' if SEMANTIC_SEARCH_AVAILABLE else '‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—Ç–æ–ª—å–∫–æ –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º)'}"
    )
    await update.message.reply_text(welcome_message)

async def consultation_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏ –∑–∞–ø–∏—Å–∏ –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é"""
    query = update.callback_query
    await query.answer()
    
    user = query.from_user
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_data = {
        "user_id": user.id,
        "username": user.username or "–ù–µ—Ç username",
        "first_name": user.first_name or "",
        "last_name": user.last_name or "",
        "timestamp": timestamp
    }
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫
    consultations = []
    if os.path.exists(CONSULTATIONS_FILE):
        with open(CONSULTATIONS_FILE, "r", encoding="utf-8") as f:
            try:
                consultations = json.load(f)
            except json.JSONDecodeError:
                consultations = []
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
    consultations.append(user_data)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    with open(CONSULTATIONS_FILE, "w", encoding="utf-8") as f:
        json.dump(consultations, f, ensure_ascii=False, indent=4)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    await query.edit_message_text(text="‚úÖ –í—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–ª–∏—Å—å –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é! –° –≤–∞–º–∏ —Å–∫–æ—Ä–æ —Å–≤—è–∂—É—Ç—Å—è.")

async def clear_list_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏ –æ—á–∏—Å—Ç–∫–∏ —Å–ø–∏—Å–∫–∞ –∑–∞—è–≤–æ–∫"""
    query = update.callback_query
    await query.answer()
    
    # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª –∑–∞—è–≤–æ–∫
    with open(CONSULTATIONS_FILE, "w", encoding="utf-8") as f:
        json.dump([], f, ensure_ascii=False, indent=4)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    await query.edit_message_text(text="‚úÖ –°–ø–∏—Å–æ–∫ –∑–∞—è–≤–æ–∫ —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω!")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    user_id = update.effective_user.id
    user_question = update.message.text.strip().lower()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã "–∑–∞—è–≤–∫–∏" —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞
    if user_id == ADMIN_USER_ID and user_question == "–∑–∞—è–≤–∫–∏":
        if not os.path.exists(CONSULTATIONS_FILE) or os.path.getsize(CONSULTATIONS_FILE) == 0:
            await update.message.reply_text("üìã –°–ø–∏—Å–æ–∫ –∑–∞—è–≤–æ–∫ –ø—É—Å—Ç.")
            return
        
        try:
            with open(CONSULTATIONS_FILE, "r", encoding="utf-8") as f:
                consultations = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            consultations = []
        
        if not consultations:
            await update.message.reply_text("üìã –°–ø–∏—Å–æ–∫ –∑–∞—è–≤–æ–∫ –ø—É—Å—Ç.")
            return
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–æ —Å–ø–∏—Å–∫–æ–º –∑–∞—è–≤–æ–∫
        message = "üìã –°–ø–∏—Å–æ–∫ –∑–∞—è–≤–æ–∫:\n\n"
        for idx, consult in enumerate(consultations, 1):
            username = consult.get('username', '–ù–µ—Ç username')
            first_name = consult.get('first_name', '')
            last_name = consult.get('last_name', '')
            timestamp = consult.get('timestamp', '')
            
            message += f"{idx}. {first_name} {last_name}\n"
            message += f"   üë§ @{username}\n"
            message += f"   ‚è∞ {timestamp}\n\n"
        
        # –°–æ–∑–¥–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—á–∏—Å—Ç–∫–∏ —Å–ø–∏—Å–∫–∞
        keyboard = [[InlineKeyboardButton("–û—á–∏—Å—Ç–∏—Ç—å —Å–ø–∏—Å–æ–∫", callback_data="clear_list")]]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        await update.message.reply_text(message, reply_markup=reply_markup)
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if user_id not in user_contexts:
        user_contexts[user_id] = {"last_answer": None}
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤
    short_answers = ['–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–∞–≥–∞', '—É–≥—É', '–µ—â–µ', '–±–æ–ª—å—à–µ', '—Ä–∞—Å—Å–∫–∞–∂–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ', '–∫–∞–∫?', '–ø–æ—á–µ–º—É?']
    if user_question in short_answers:
        last_answer = user_contexts[user_id]["last_answer"]
        if last_answer:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –∫–Ω–æ–ø–∫—É –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏
            if "[add_button]" in last_answer:
                clean_answer = last_answer.replace("[add_button]", "").strip()
                keyboard = [[InlineKeyboardButton("–ó–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é", callback_data="consultation")]]
                reply_markup = InlineKeyboardMarkup(keyboard)
                await update.message.reply_text(clean_answer, reply_markup=reply_markup)
            else:
                await update.message.reply_text(last_answer)
            return
    
    # –ü–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞
    answer = find_best_match(update.message.text, kb_index)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ—á–∏—â–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –±–µ–∑ –º–∞—Ä–∫–µ—Ä–∞)
    clean_answer = answer.replace("[add_button]", "").strip()
    user_contexts[user_id]["last_answer"] = clean_answer
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –∫–Ω–æ–ø–∫—É –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏
    if "[add_button]" in answer:
        keyboard = [[InlineKeyboardButton("–ó–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é", callback_data="consultation")]]
        reply_markup = InlineKeyboardMarkup(keyboard)
        await update.message.reply_text(clean_answer, reply_markup=reply_markup)
    else:
        await update.message.reply_text(clean_answer)

async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫"""
    print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {context.error}")
    if update and hasattr(update, 'message'):
        await update.message.reply_text(
            "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. "
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –µ–≥–æ."
        )

def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞"""
    global kb_index
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞ –∏–∑ .env —Ñ–∞–π–ª–∞
    token = os.getenv("BOT_TOKEN")
    if not token:
        raise ValueError("–¢–æ–∫–µ–Ω –±–æ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ. –£–∫–∞–∂–∏—Ç–µ BOT_TOKEN=–≤–∞—à_—Ç–æ–∫–µ–Ω")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    try:
        kb = load_knowledge_base('main.json')
        kb_index = preprocess_knowledge_base(kb)
        print("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {str(e)}")
        raise
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    application = Application.builder().token(token).build()
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    application.add_handler(CallbackQueryHandler(consultation_callback, pattern="consultation"))
    application.add_handler(CallbackQueryHandler(clear_list_callback, pattern="clear_list"))
    application.add_error_handler(error_handler)
    
    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    application.run_polling()

if __name__ == "__main__":
    main()