import json
import re
import os
from pathlib import Path
from typing import List, Set, Dict, Tuple, Optional, Any
from collections import deque
from datetime import datetime, timedelta

import numpy as np
import pymorphy2
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from config import (
    RUSSIAN_STOPWORDS, SYNONYMS, logger,
    MIN_FULLTEXT_SCORE, MAX_HISTORY_LENGTH, INACTIVITY_LIMIT_HOURS
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
morph = pymorphy2.MorphAnalyzer()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
kb_index = None  # –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ main.py
user_contexts: Dict[int, dict] = {}


# ====================== –†–∞–±–æ—Ç–∞ —Å JSON ======================

def load_json(file_path: str) -> list:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ JSON-—Ñ–∞–π–ª–∞"""
    if not os.path.exists(file_path):
        return []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        logger.error(f"Error loading {file_path}: {e}")
        return []

def save_json(file_path: str, data: list) -> None:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON-—Ñ–∞–π–ª–∞"""
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    except IOError as e:
        logger.error(f"Error saving {file_path}: {e}")


# ====================== NLP —Ñ—É–Ω–∫—Ü–∏–∏ ======================

def preprocess_question(question: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –≤–≤–æ–¥–Ω—ã–µ —Ñ—Ä–∞–∑—ã (–∞ –µ—Å–ª–∏, —Å–∫–∞–∂–∏ –∏ —Ç.–ø.)"""
    patterns = [
        r'^–∞ –µ—Å–ª–∏\s+', r'^—á—Ç–æ –µ—Å–ª–∏\s+', r'^—á—Ç–æ –±—É–¥–µ—Ç –µ—Å–ª–∏\s+',
        r'^–º–æ–∂–Ω–æ –ª–∏\s+', r'^–∞ —á—Ç–æ –µ—Å–ª–∏\s+', r'^–µ—Å–ª–∏ —è\s+',
        r'^–∞\s+', r'^–Ω—É\s+', r'^—Å–∫–∞–∂–∏\s+', r'^—Ä–∞—Å—Å–∫–∞–∂–∏\s+', r'^–æ–±—ä—è—Å–Ω–∏\s+'
    ]
    cleaned = question.lower()
    for pattern in patterns:
        cleaned = re.sub(pattern, '', cleaned)
    return cleaned.strip()

def preprocess_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å—Å—ã–ª–æ–∫, email –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è"""
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    return re.sub(r'[^\w\s]', ' ', text.lower().strip())

def lemmatize_word(word: str) -> str:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if not hasattr(lemmatize_word, 'cache'):
        lemmatize_word.cache = {}
    if word in lemmatize_word.cache:
        return lemmatize_word.cache[word]
    parsed = morph.parse(word)[0]
    lemma = parsed.normal_form
    lemmatize_word.cache[word] = lemma
    return lemma

def lemmatize_sentence(text: str) -> str:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É–¥–∞–ª–µ–Ω–∏–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤"""
    text = re.sub(r'[?!.]', '', text)
    words = preprocess_text(text).split()
    lemmas = [lemmatize_word(word) for word in words
              if word not in RUSSIAN_STOPWORDS and len(word) > 2]
    return " ".join(lemmas)

def expand_with_synonyms(keywords: Set[str]) -> Set[str]:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
    expanded = set(keywords)
    for word in keywords:
        for base, synonyms in SYNONYMS.items():
            if word == base or any(word == syn for syn in synonyms):
                expanded.update([base] + synonyms)
    return expanded

def extract_keywords(text: str, use_synonyms: bool = True) -> Set[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    cleaned = preprocess_text(text)
    words = cleaned.split()
    keywords = {lemmatize_word(word) for word in words
                if len(word) > 2 and word not in RUSSIAN_STOPWORDS}
    if use_synonyms and keywords:
        keywords = expand_with_synonyms(keywords)
    return keywords

def calculate_keyword_match_score(user_keywords: Set[str], item_keywords: Set[str],
                                  user_question: str, original_keywords: List[str]) -> float:
    """–û—Ü–µ–Ω–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    common = user_keywords.intersection(item_keywords)
    base_score = len(common) * 2
    question_lower = preprocess_text(user_question)
    phrase_bonus = 0
    for kw in original_keywords:
        kw_lower = preprocess_text(kw)
        if kw_lower in question_lower:
            phrase_bonus += len(kw_lower.split()) * 3
    return base_score + phrase_bonus


# ====================== –ö–ª–∞—Å—Å –∏–Ω–¥–µ–∫—Å–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ======================

class KBIndex:
    def __init__(self):
        self.items = []                # —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ "context", "keywords", "original_keywords"
        self.contexts = []              # —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤
        self.tfidf_vectorizer = None
        self.tfidf_labeled_matrix = None
        self.raw_tfidf_vectorizer = None
        self.tfidf_raw_matrix = None
        self.all_keywords_list = []     # –¥–ª—è –Ω–µ—á—ë—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞

    def build_tfidf_index(self, contexts: List[str]):
        """–°—Ç—Ä–æ–∏—Ç –¥–≤–∞ TF-IDF –∏–Ω–¥–µ–∫—Å–∞: –ø–æ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º –∏ –ø–æ —Å—ã—Ä—ã–º"""
        # –ò–Ω–¥–µ–∫—Å –ø–æ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º (–¥–ª—è –ª—É—á—à–µ–≥–æ semantic matching)
        self.tfidf_vectorizer = TfidfVectorizer(
            lowercase=True,
            stop_words=list(RUSSIAN_STOPWORDS),
            ngram_range=(1, 3),
            max_features=3000
        )
        lemmatized = [lemmatize_sentence(ctx) for ctx in contexts]
        self.tfidf_labeled_matrix = self.tfidf_vectorizer.fit_transform(lemmatized)

        # –ò–Ω–¥–µ–∫—Å –ø–æ —Å—ã—Ä—ã–º —Ç–µ–∫—Å—Ç–∞–º (–¥–ª—è —É—á—ë—Ç–∞ —Ç–æ—á–Ω—ã—Ö —Ñ—Ä–∞–∑)
        self.raw_tfidf_vectorizer = TfidfVectorizer(
            lowercase=True,
            stop_words=list(RUSSIAN_STOPWORDS),
            ngram_range=(1, 2),
            max_features=2000
        )
        self.tfidf_raw_matrix = self.raw_tfidf_vectorizer.fit_transform(contexts)

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –Ω–µ—á—ë—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        all_kw = set()
        for item in self.items:
            all_kw.update(item["original_keywords"])
        self.all_keywords_list = list(all_kw)

    def keyword_search(self, user_question: str, top_k: int = 3) -> List[dict]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –±–æ–Ω—É—Å–∞–º–∏)"""
        user_keywords = extract_keywords(user_question)
        if not user_keywords:
            return []

        scored = []
        for idx, item in enumerate(self.items):
            score = calculate_keyword_match_score(
                user_keywords, item["keywords"], user_question, item["original_keywords"]
            )
            if score > 0:
                scored.append({
                    "context": item["context"],
                    "score": score,
                    "index": idx
                })
        scored.sort(key=lambda x: x["score"], reverse=True)
        return scored[:top_k]

    def fulltext_search(self, query: str, top_k: int = 3) -> List[dict]:
        """TF-IDF –ø–æ–∏—Å–∫ (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏ —Å—ã—Ä–æ–≥–æ)"""
        if self.tfidf_vectorizer is None or self.tfidf_labeled_matrix is None:
            return []

        try:
            # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            query_lemma = lemmatize_sentence(query)
            query_vec = self.tfidf_vectorizer.transform([query_lemma])
            labeled_sim = cosine_similarity(query_vec, self.tfidf_labeled_matrix)[0]

            # –°—ã—Ä–æ–π –∑–∞–ø—Ä–æ—Å
            raw_vec = self.raw_tfidf_vectorizer.transform([query])
            raw_sim = cosine_similarity(raw_vec, self.tfidf_raw_matrix)[0]

            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
            combined = 0.7 * labeled_sim + 0.3 * raw_sim
            top_indices = np.argsort(combined)[::-1][:top_k]

            results = []
            for idx in top_indices:
                score = combined[idx]
                if score > MIN_FULLTEXT_SCORE:
                    results.append({
                        "context": self.contexts[idx],
                        "score": float(score),
                        "index": int(idx)
                    })
            return results
        except Exception as e:
            logger.error(f"Fulltext search error: {e}")
            return []

    def is_valid_index(self, idx: int) -> bool:
        return 0 <= idx < len(self.items)


def preprocess_knowledge_base(knowledge_base: list) -> KBIndex:
    """–°–æ–∑–¥–∞—ë—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä KBIndex –∏–∑ —Å—ã—Ä–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (main.json)"""
    kb_index = KBIndex()
    processed_items = []
    contexts = [item["context"] for item in knowledge_base]

    for item in knowledge_base:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
        processed_keywords = set()
        for keyword in item["keywords"]:
            for word in re.split(r'\s+', preprocess_text(keyword)):
                if len(word) > 2 and word not in RUSSIAN_STOPWORDS:
                    processed_keywords.add(lemmatize_word(word))
        processed_items.append({
            "context": item["context"],
            "keywords": processed_keywords,
            "original_keywords": item["keywords"]
        })

    kb_index.items = processed_items
    kb_index.contexts = contexts
    kb_index.build_tfidf_index(contexts)
    return kb_index


def search_knowledge_base(user_question: str, kb_index: KBIndex) -> Tuple[Optional[str], float, List[dict]]:
    """
    –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫: –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã keyword_search –∏ fulltext_search
    —Å –≤–µ—Å–∞–º–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç (–µ—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤—ã—Å–æ–∫–∞) –∏–ª–∏ —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.
    """
    cleaned = preprocess_question(user_question)

    kw_results = kb_index.keyword_search(cleaned, top_k=5)
    ft_results = kb_index.fulltext_search(cleaned, top_k=5)

    # –ï—Å–ª–∏ –ø–æ –æ—á–∏—â–µ–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π
    if not kw_results and not ft_results:
        kw_results = kb_index.keyword_search(user_question, top_k=5)
        ft_results = kb_index.fulltext_search(user_question, top_k=5)

    # –°–æ–±–∏—Ä–∞–µ–º –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º
    combined = {}
    for res in kw_results:
        combined.setdefault(res["index"], 0)
        combined[res["index"]] += res["score"] * 0.6

    for res in ft_results:
        combined.setdefault(res["index"], 0)
        combined[res["index"]] += res["score"] * 50 * 0.4  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º TF-IDF –¥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö –≤–µ–ª–∏—á–∏–Ω

    if not combined:
        return None, 0.0, []

    sorted_items = sorted(combined.items(), key=lambda x: x[1], reverse=True)
    candidates = []
    for idx, score in sorted_items[:3]:
        topic = kb_index.items[idx]["original_keywords"][0] if kb_index.items[idx]["original_keywords"] else "–¢–µ–º–∞"
        candidates.append({
            "index": idx,
            "score": score,
            "topic": topic,
            "context": kb_index.items[idx]["context"]
        })

    best_idx, best_score = sorted_items[0]
    return kb_index.items[best_idx]["context"], best_score, candidates


# ====================== –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ======================

def get_user_context(user_id: int) -> dict:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —Å–æ–∑–¥–∞—ë—Ç –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
    if user_id not in user_contexts:
        user_contexts[user_id] = {
            "history": deque(maxlen=MAX_HISTORY_LENGTH),
            "last_activity": datetime.now(),
            "question_index_map": {},   # —Å–≤—è–∑—å –∏–Ω–¥–µ–∫—Å–∞ –æ—Ç–≤–µ—Ç–∞ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –≤–æ–ø—Ä–æ—Å–æ–º
            "favorites": set()           # –Ω–æ–≤—ã–µ: –∏–∑–±—Ä–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (–∏–Ω–¥–µ–∫—Å—ã)
        }
    return user_contexts[user_id]

def update_user_activity(user_id: int) -> None:
    get_user_context(user_id)["last_activity"] = datetime.now()

def cleanup_inactive_users() -> None:
    now = datetime.now()
    to_delete = [
        uid for uid, ctx in user_contexts.items()
        if now - ctx.get("last_activity", now) > timedelta(hours=INACTIVITY_LIMIT_HOURS)
    ]
    for uid in to_delete:
        del user_contexts[uid]

def save_question_for_answer(user_id: int, answer_index: int, question: str) -> None:
    ctx = get_user_context(user_id)
    ctx["question_index_map"][answer_index] = question

def get_question_for_answer(user_id: int, answer_index: int) -> str:
    ctx = get_user_context(user_id)
    return ctx.get("question_index_map", {}).get(answer_index, "???")

def add_favorite(user_id: int, answer_index: int) -> None:
    ctx = get_user_context(user_id)
    ctx["favorites"].add(answer_index)

def remove_favorite(user_id: int, answer_index: int) -> None:
    ctx = get_user_context(user_id)
    ctx["favorites"].discard(answer_index)

def get_favorites(user_id: int) -> List[int]:
    return list(get_user_context(user_id)["favorites"])

def get_contextual_question(user_id: int, current_question: str) -> str:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ —É—Ç–æ—á–Ω—è—é—â–∏–π"""
    ctx = get_user_context(user_id)
    history = ctx.get("history", [])
    if not history:
        return current_question

    context_markers = ['–∞', '–∞ –µ—Å—Ç—å', '–∞ –∫–∞–∫', '–∞ —Å–∫–æ–ª—å–∫–æ', '–∞ —Å–∫–∏–¥–∫–∏', '–∞ —Ä–∞—Å—Å—Ä–æ—á–∫–∞', '–∞ –¥–æ–∫—É–º–µ–Ω—Ç']
    q_lower = current_question.lower()
    if len(q_lower) < 20 or any(marker in q_lower for marker in context_markers):
        last_msg = list(history)[-1]
        return f"{last_msg} {current_question}"
    return current_question


# ====================== –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏ –∫–Ω–æ–ø–æ–∫ ======================

def extract_links_and_buttons(text: str) -> Tuple[str, List[List[Any]]]:
    """–ò—â–µ—Ç —Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –∏ —Å–ø–∏—Å–æ–∫ —Ä—è–¥–æ–≤ InlineKeyboardButton"""
    from telegram import InlineKeyboardButton  # –∏–º–ø–æ—Ä—Ç –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏–∫–ª–∏—á–µ—Å–∫–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞

    buttons = []
    url_pattern = r'(https?://[^\s<]+)'
    urls = re.findall(url_pattern, text)

    if urls:
        for raw_url in set(urls):
            clean_url = raw_url.replace("[add_button]", "").strip('.,;:!?()"\'[]{}')
            if not clean_url:
                continue

            # –ö—Ä–∞—Å–∏–≤—ã–µ –ø–æ–¥–ø–∏—Å–∏
            label = "üîó –û—Ç–∫—Ä—ã—Ç—å —Å—Å—ã–ª–∫—É"
            if "roadmap" in clean_url.lower():
                label = "üó∫ –î–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞"
            elif "Business-card" in clean_url or "avick23.github.io" in clean_url:
                label = "üë§ –û –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ"
            elif "t.me" in clean_url:
                label = "üí¨ Telegram"
            elif "calendar" in clean_url.lower():
                label = "üìÖ –í—ã–±—Ä–∞—Ç—å –≤—Ä–µ–º—è"

            buttons.append([InlineKeyboardButton(label, url=clean_url)])

        # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        clean_text = re.sub(url_pattern, '', text).strip()
        clean_text = re.sub(r'\s+\.', '.', clean_text)
        clean_text = re.sub(r'\(\s*\)', '', clean_text).strip()
        return clean_text, buttons

    return text, []