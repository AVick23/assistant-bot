import json
import re
import math
import os
import logging
from typing import List, Dict, Set, Tuple, Optional
from datetime import datetime, timedelta
from collections import deque, Counter
from rank_bm25 import BM25Okapi
import numpy as np
from config import (
    FILES, RUSSIAN_STOPWORDS, SYNONYMS, morph,
    logger, SETTINGS, URLS
)

# ============================================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï
# ============================================================
_kb_index = None  # ‚úÖ –ü—Ä–∏–≤–∞—Ç–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
user_contexts: Dict[int, dict] = {}

# ============================================================
# –ì–ï–¢–¢–ï–† –î–õ–Ø KB_INDEX (—Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å None)
# ============================================================
def get_kb_index() -> 'KBIndex':
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    return _kb_index

# ============================================================
# –†–ê–ë–û–¢–ê –° JSON
# ============================================================
def load_json(file_path: str) -> list:
    if not os.path.exists(file_path):
        return []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading {file_path}: {e}")
        return []

def save_json(file_path: str, data: list) -> None:
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logger.error(f"Error saving {file_path}: {e}")

# ============================================================
# NLP –£–¢–ò–õ–ò–¢–´
# ============================================================
def preprocess_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    return re.sub(r'[^\w\s]', ' ', text.lower().strip())

def lemmatize_word(word: str) -> str:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if not hasattr(lemmatize_word, 'cache'):
        lemmatize_word.cache = {}
    if word in lemmatize_word.cache:
        return lemmatize_word.cache[word]
    try:
        parsed = morph.parse(word)[0]
        lemma = parsed.normal_form
        lemmatize_word.cache[word] = lemma
        return lemma
    except:
        return word

def lemmatize_sentence(text: str) -> str:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
    text = re.sub(r'[?!.]', '', text)
    words = preprocess_text(text).split()
    lemmas = [lemmatize_word(w) for w in words if w not in RUSSIAN_STOPWORDS and len(w) > 2]
    return " ".join(lemmas)

def expand_with_synonyms(keywords: Set[str]) -> Set[str]:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
    expanded = set(keywords)
    for word in keywords:
        for base, syns in SYNONYMS.items():
            if word == base or word in syns:
                expanded.add(base)
                expanded.update(syns)
    return expanded

# ============================================================
# –ê–í–¢–û-–ì–ï–ù–ï–†–ê–¶–ò–Ø KEYWORDS
# ============================================================
def auto_generate_keywords(context: str, max_kw: int = None) -> List[str]:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    """
    if max_kw is None:
        max_kw = SETTINGS.get('max_keywords', 30)
    
    keywords = set()
    
    # 1. –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å—É—Ç—å)
    sentences = re.split(r'[.!?]', context)[:3]
    important_text = ' '.join(sentences)
    words = preprocess_text(important_text).split()
    
    for word in words:
        if len(word) > 2 and word not in RUSSIAN_STOPWORDS:
            try:
                parsed = morph.parse(word)[0]
                if any(tag in parsed.tag for tag in ['NOUN', 'ADJF', 'INFN', 'VERB']):
                    keywords.add(parsed.normal_form)
            except:
                keywords.add(word)
    
    # 2. –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –∏–∑ –í–°–ï–ì–û –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    full_words = preprocess_text(context).split()
    for word in full_words:
        if len(word) > 2 and word not in RUSSIAN_STOPWORDS:
            try:
                parsed = morph.parse(word)[0]
                if any(tag in parsed.tag for tag in ['NOUN', 'ADJF', 'INFN', 'VERB']):
                    keywords.add(parsed.normal_form)
            except:
                keywords.add(word)
    
    # 3. –î–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–∑—ã (2-3 —Å–ª–æ–≤–∞)
    word_list = preprocess_text(context).split()
    for i in range(len(word_list) - 1):
        phrase_2 = f"{word_list[i]} {word_list[i+1]}"
        if len(phrase_2) > 5:
            keywords.add(phrase_2)
    
    for i in range(len(word_list) - 2):
        phrase_3 = f"{word_list[i]} {word_list[i+1]} {word_list[i+2]}"
        if len(phrase_3) > 8:
            keywords.add(phrase_3)
    
    # 4. –†–∞—Å—à–∏—Ä—è–µ–º —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    keywords = expand_with_synonyms(keywords)
    
    # 5. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
    ctx_lower = context.lower()
    if '[add_button]' in context:
        keywords.update(['–∑–∞–ø–∏—Å–∞—Ç—å—Å—è', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', '–∑–∞—è–≤–∫–∞', '–∑–∞–ø–∏—Å—å'])
    if 'http' in context:
        keywords.update(['—Å—Å—ã–ª–∫–∞', '—Å–∞–π—Ç', '—Ä–µ—Å—É—Ä—Å', '–º–∞—Ç–µ—Ä–∏–∞–ª—ã'])
    if any(x in ctx_lower for x in ['—Ü–µ–Ω–∞', '—Ä—É–±', '‚ÇΩ', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ç–∞—Ä–∏—Ñ']):
        keywords.update(['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ç–∞—Ä–∏—Ñ', '–æ–ø–ª–∞—Ç–∞', '—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç', '–ø–ª–∞—Ç–Ω–æ'])
    if any(x in ctx_lower for x in ['python', '–ø–∏—Ç–æ–Ω', '–ø–∞–π—Ç–æ–Ω']):
        keywords.update(['python', '–ø–∏—Ç–æ–Ω', '–ø–∞–π—Ç–æ–Ω', '—è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è'])
    if any(x in ctx_lower for x in ['–≥—Ä—É–ø–ø–∞', '–º–∏–Ω–∏-–≥—Ä—É–ø–ø–∞', '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ']):
        keywords.update(['–≥—Ä—É–ø–ø–∞', '–º–∏–Ω–∏-–≥—Ä—É–ø–ø–∞', '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ', '—Ñ–æ—Ä–º–∞—Ç'])
    
    # 6. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞–∫—Å–∏–º—É–º keywords
    return list(keywords)[:max_kw]

def update_keywords_in_db(kb_data: list, force_regenerate: bool = None) -> int:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç keywords –≤ main.json –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ.
    –ò–∑–º–µ–Ω—è–µ—Ç kb_data in-place.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.
    """
    if force_regenerate is None:
        force_regenerate = SETTINGS.get('force_regenerate', True)
    
    updated_count = 0
    
    for item in kb_data:
        should_update = force_regenerate or not item.get('keywords') or len(item.get('keywords', [])) < 5
        
        if should_update:
            new_kws = auto_generate_keywords(item['context'])
            item['keywords'] = new_kws
            updated_count += 1
    
    if updated_count > 0:
        save_json(FILES['kb'], kb_data)
        logger.info(f"‚úÖ –ê–≤—Ç–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è keywords: –æ–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} –∑–∞–ø–∏—Å–µ–π.")
    else:
        logger.info("‚úÖ –í—Å–µ keywords –≤ –ø–æ—Ä—è–¥–∫–µ.")
    
    return updated_count

# ============================================================
# –ö–õ–ê–°–° –ò–ù–î–ï–ö–°–ê (HYBRID SEARCH: Keywords + Context)
# ============================================================
class KBIndex:
    def __init__(self, items: list):
        self.items = items
        self.contexts = [item['context'] for item in items] if items else []
        
        # ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è BM25
        searchable_texts = []
        for item in items:
            text = item['context']
            if item.get('keywords'):
                text += " " + " ".join(item['keywords'])
            searchable_texts.append(text)
        
        self.tokenized_contexts = [lemmatize_sentence(t).split() for t in searchable_texts] if searchable_texts else []
        self.bm25 = BM25Okapi(self.tokenized_contexts) if self.tokenized_contexts else None
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        self.item_keywords = [set(item.get('keywords', [])) for item in items]
        
        # –í—Å–µ keywords –¥–ª—è –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.all_keywords = []
        for item in items:
            self.all_keywords.extend(item.get('keywords', []))
        self.all_keywords = list(set(self.all_keywords))
    
    def search(self, query: str, top_k: int = 5, user_context: Optional[dict] = None) -> List[dict]:
        """
        ‚úÖ –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: BM25 + Keywords + –ö–æ–Ω—Ç–µ–∫—Å—Ç –±–µ—Å–µ–¥—ã
        """
        if not self.items or not self.bm25:
            return []
        
        query_lemmas = lemmatize_sentence(query).split()
        query_lower = preprocess_text(query)
        
        # --- –®–∞–≥ 1: –ë–∞–∑–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏ BM25 ---
        bm25_scores = self.bm25.get_scores(query_lemmas)
        
        # --- –®–∞–≥ 2: –ë–æ–Ω—É—Å—ã –∑–∞ Keywords ---
        final_scores = bm25_scores.copy()
        
        for idx in range(len(self.items)):
            score_boost = 0.0
            
            for kw in self.item_keywords[idx]:
                if len(kw.split()) > 1 and kw.lower() in query_lower:
                    score_boost += 5.0
                elif kw.lower() in query_lower:
                    score_boost += 2.0
            
            # ‚úÖ –ë–æ–Ω—É—Å –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –±–µ—Å–µ–¥—ã
            if user_context:
                history = user_context.get('history', [])
                for hist_msg in history[-5:]:
                    hist_lemmas = set(lemmatize_sentence(hist_msg).split())
                    query_lemmas_set = set(query_lemmas)
                    overlap = len(hist_lemmas & query_lemmas_set)
                    if overlap > 0:
                        score_boost += overlap * 0.5
            
            final_scores[idx] += score_boost
        
        # --- –®–∞–≥ 3: –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –æ—Ç–±–æ—Ä ---
        top_indices = np.argsort(final_scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            score = final_scores[idx]
            if score > 0.5:
                results.append({
                    "index": int(idx),
                    "score": float(score),
                    "context": self.contexts[idx],
                    "topic": self.items[idx].get('keywords', ['–¢–µ–º–∞'])[0] if self.items[idx].get('keywords') else '–¢–µ–º–∞'
                })
        
        return results
    
    def is_valid_index(self, idx: int) -> bool:
        return 0 <= idx < len(self.items)

# ============================================================
# –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô
# ============================================================
def initialize_kb() -> KBIndex:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
    ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–≤–∞—Ç–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é _kb_index
    """
    global _kb_index
    
    # ‚úÖ –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞
    kb_data = load_json(FILES['kb'])
    
    if not kb_data:
        logger.error("‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞ –∏–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        _kb_index = KBIndex([])
        return _kb_index
    
    # –û–±–Ω–æ–≤–ª—è–µ–º keywords (–ø–µ—Ä–µ–¥–∞—ë–º kb_data, —Ñ—É–Ω–∫—Ü–∏—è –∏–∑–º–µ–Ω–∏—Ç –µ–≥–æ in-place)
    update_keywords_in_db(kb_data)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    _kb_index = KBIndex(kb_data)
    
    logger.info(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(_kb_index.items)} –∑–∞–ø–∏—Å–µ–π")
    return _kb_index

# ============================================================
# –ü–û–ò–°–ö –ò –ö–û–ù–¢–ï–ö–°–¢
# ============================================================
def get_user_context(user_id: int) -> dict:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    if user_id not in user_contexts:
        user_contexts[user_id] = {
            "history": deque(maxlen=SETTINGS['max_history']),
            "last_activity": datetime.now(),
            "question_index_map": {}
        }
    return user_contexts[user_id]

def update_user_activity(user_id: int):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    ctx = get_user_context(user_id)
    ctx["last_activity"] = datetime.now()

def save_question_for_answer(user_id: int, ans_idx: int, question: str):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
    ctx = get_user_context(user_id)
    ctx["question_index_map"][ans_idx] = question

def get_question_for_answer(user_id: int, ans_idx: int) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
    ctx = get_user_context(user_id)
    return ctx.get("question_index_map", {}).get(ans_idx, "???")

def save_message_to_history(user_id: int, message: str, is_user: bool = True):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é"""
    ctx = get_user_context(user_id)
    prefix = "User: " if is_user else "Bot: "
    ctx["history"].append(f"{prefix}{message}")

def get_contextual_question(user_id: int, current_question: str) -> str:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
    ctx = get_user_context(user_id)
    history = ctx.get("history", [])
    
    if not history:
        return current_question
    
    context_markers = ['–∞', '–∞ –µ—Å—Ç—å', '–∞ –∫–∞–∫', '–∞ —Å–∫–æ–ª—å–∫–æ', '–∞ —Å–∫–∏–¥–∫–∏', '–∞ —Ä–∞—Å—Å—Ä–æ—á–∫–∞', '–∞ –¥–æ–∫—É–º–µ–Ω—Ç', 
                       '–∏', '—Ç–æ–∂–µ', '—Ç–∞–∫–∂–µ', '–µ—â–µ', '–µ—â—ë', '–ø—Ä–æ–¥–æ–ª–∂–∏', '–¥–∞–ª–µ–µ']
    q_lower = current_question.lower()
    
    if len(q_lower) < 20 or any(marker in q_lower for marker in context_markers):
        recent_history = list(history)[-3:] if len(history) >= 3 else list(history)
        history_context = " ".join(recent_history)
        return f"{history_context} {current_question}"
    
    return current_question

def cleanup_inactive_users():
    """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –æ—Ç –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    now = datetime.now()
    to_delete = [
        uid for uid, ctx in user_contexts.items()
        if now - ctx.get("last_activity", now) > timedelta(hours=SETTINGS['inactivity_hours'])
    ]
    for uid in to_delete:
        del user_contexts[uid]

# ============================================================
# –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –°–°–´–õ–û–ö –î–õ–Ø –ö–ù–û–ü–û–ö
# ============================================================
def extract_links_and_buttons(text: str) -> Tuple[str, List[List[dict]]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏ —Å–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –∫–Ω–æ–ø–æ–∫"""
    buttons = []
    url_pattern = r'(https?://[^\s<]+)'
    urls = re.findall(url_pattern, text)
    
    if urls:
        for raw_url in set(urls):
            clean_url = raw_url.replace("[add_button]", "").strip('.,;:!?() "\'[]{}')
            if not clean_url:
                continue
            
            label = "üîó –°—Å—ã–ª–∫–∞"
            if "roadmap" in clean_url.lower():
                label = "üó∫ –î–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞"
            elif "Business-card" in clean_url:
                label = "üë§ –û –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ"
            elif "calendar" in clean_url.lower():
                label = "üìÖ –í—ã–±—Ä–∞—Ç—å –≤—Ä–µ–º—è"
            
            buttons.append([{"text": label, "url": clean_url}])
        
        clean_text = re.sub(url_pattern, '', text).strip()
        return clean_text, buttons
    
    return text, []