import json
import re
import os
import logging
from typing import List, Dict, Set, Tuple, Optional
from datetime import datetime, timedelta
from collections import deque
from rank_bm25 import BM25Okapi
import numpy as np
from config import (
    FILES, RUSSIAN_STOPWORDS, SYNONYMS, morph,
    logger, SETTINGS, URLS
)

# ============================================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï
# ============================================================
_kb_index = None
user_contexts: Dict[int, dict] = {}

# ============================================================
# –ì–ï–¢–¢–ï–† –î–õ–Ø KB_INDEX
# ============================================================
def get_kb_index() -> 'KBIndex':
    return _kb_index

# ============================================================
# –†–ê–ë–û–¢–ê –° JSON
# ============================================================
def load_json(file_path: str) -> list:
    if not os.path.exists(file_path):
        return []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading {file_path}: {e}")
        return []

def save_json(file_path: str, data: list) -> None:
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logger.error(f"Error saving {file_path}: {e}")

# ============================================================
# NLP –£–¢–ò–õ–ò–¢–´
# ============================================================
def preprocess_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    return re.sub(r'[^\w\s]', ' ', text.lower().strip())

def lemmatize_word(word: str) -> str:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if not hasattr(lemmatize_word, 'cache'):
        lemmatize_word.cache = {}
    if word in lemmatize_word.cache:
        return lemmatize_word.cache[word]
    try:
        parsed = morph.parse(word)[0]
        lemma = parsed.normal_form
        lemmatize_word.cache[word] = lemma
        return lemma
    except:
        return word

def lemmatize_sentence(text: str) -> str:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
    text = re.sub(r'[?!.]', '', text)
    words = preprocess_text(text).split()
    lemmas = [lemmatize_word(w) for w in words if w not in RUSSIAN_STOPWORDS and len(w) > 2]
    return " ".join(lemmas)

def expand_query_with_synonyms(keywords: Set[str]) -> Set[str]:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
    expanded = set(keywords)
    for word in keywords:
        for base, syns in SYNONYMS.items():
            if word == base or word in syns:
                expanded.add(base)
                expanded.update(syns)
    return expanded

# ============================================================
# –ö–õ–ê–°–° –ò–ù–î–ï–ö–°–ê (HYBRID SEARCH: Keywords + Context)
# ============================================================
class KBIndex:
    def __init__(self, items: list):
        self.items = items
        self.contexts = [item['context'] for item in items] if items else []
        
        # ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è BM25
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º context –ò keywords –∏–∑ JSON
        searchable_texts = []
        for item in items:
            text = item['context']
            # –î–æ–±–∞–≤–ª—è–µ–º keywords –∫ —Ç–µ–∫—Å—Ç—É –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏, —á—Ç–æ–±—ã BM25 –∏—Ö —É—á–∏—Ç—ã–≤–∞–ª
            if item.get('keywords'):
                text += " " + " ".join(item['keywords'])
            searchable_texts.append(text)
        
        self.tokenized_contexts = [lemmatize_sentence(t).split() for t in searchable_texts] if searchable_texts else []
        self.bm25 = BM25Okapi(self.tokenized_contexts) if self.tokenized_contexts else None
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥–∞
        self.item_keywords = [set(item.get('keywords', [])) for item in items]
    
    def search(self, query: str, top_k: int = 5, user_context: Optional[dict] = None) -> List[dict]:
        """
        ‚úÖ –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: BM25 + Keywords + –ö–æ–Ω—Ç–µ–∫—Å—Ç –±–µ—Å–µ–¥—ã
        """
        if not self.items or not self.bm25:
            return []
        
        query_lemmas = lemmatize_sentence(query).split()
        query_lower = preprocess_text(query)
        
        # --- –®–∞–≥ 1: –ë–∞–∑–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏ BM25 ---
        bm25_scores = self.bm25.get_scores(query_lemmas)
        
        # --- –®–∞–≥ 2: –ë–æ–Ω—É—Å—ã –∑–∞ Keywords (–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ) ---
        final_scores = bm25_scores.copy()
        
        for idx in range(len(self.items)):
            score_boost = 0.0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∑–∞–ø–∏—Å–∏
            for kw in self.item_keywords[idx]:
                # –ï—Å–ª–∏ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ (—Ñ—Ä–∞–∑–∞) –Ω–∞–π–¥–µ–Ω–æ –≤ –∑–∞–ø—Ä–æ—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                if len(kw.split()) > 1 and kw.lower() in query_lower:
                    score_boost += 5.0  # –ë–æ–ª—å—à–æ–π –±–æ–Ω—É—Å –∑–∞ —Ñ—Ä–∞–∑—É
                elif kw.lower() in query_lower:
                    score_boost += 2.0  # –ë–æ–Ω—É—Å –∑–∞ —Å–ª–æ–≤–æ
            
            # ‚úÖ –ë–æ–Ω—É—Å –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –±–µ—Å–µ–¥—ã
            if user_context:
                history = user_context.get('history', [])
                history_list = list(history)
                for hist_msg in history_list[-5:]:
                    hist_lemmas = set(lemmatize_sentence(hist_msg).split())
                    query_lemmas_set = set(query_lemmas)
                    overlap = len(hist_lemmas & query_lemmas_set)
                    if overlap > 0:
                        score_boost += overlap * 0.5
            
            final_scores[idx] += score_boost
        
        # --- –®–∞–≥ 3: –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –æ—Ç–±–æ—Ä ---
        top_indices = np.argsort(final_scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            score = final_scores[idx]
            if score > 0.5:
                results.append({
                    "index": int(idx),
                    "score": float(score),
                    "context": self.contexts[idx],
                    "topic": self.items[idx].get('keywords', ['–¢–µ–º–∞'])[0] if self.items[idx].get('keywords') else '–¢–µ–º–∞'
                })
        
        return results
    
    def is_valid_index(self, idx: int) -> bool:
        return 0 <= idx < len(self.items)

# ============================================================
# –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô
# ============================================================
def initialize_kb() -> KBIndex:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
    ‚úÖ –ü—Ä–æ—Å—Ç–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç JSON –∏ —Å–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å. –ù–∏—á–µ–≥–æ –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç.
    """
    global _kb_index
    
    kb_data = load_json(FILES['kb'])
    
    if not kb_data:
        logger.error("‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞ –∏–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        _kb_index = KBIndex([])
        return _kb_index
    
    # ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è keywords (–ø—Ä–æ—Å—Ç–æ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
    count_with_kw = sum(1 for item in kb_data if item.get('keywords'))
    if count_with_kw < len(kb_data):
        logger.warning(f"‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: {len(kb_data) - count_with_kw} –∑–∞–ø–∏—Å–µ–π –Ω–µ –∏–º–µ—é—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤!")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    _kb_index = KBIndex(kb_data)
    
    logger.info(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(_kb_index.items)} –∑–∞–ø–∏—Å–µ–π")
    return _kb_index

# ============================================================
# –ü–û–ò–°–ö –ò –ö–û–ù–¢–ï–ö–°–¢
# ============================================================
def get_user_context(user_id: int) -> dict:
    if user_id not in user_contexts:
        user_contexts[user_id] = {
            "history": deque(maxlen=SETTINGS['max_history']),
            "last_activity": datetime.now(),
            "question_index_map": {}
        }
    return user_contexts[user_id]

def update_user_activity(user_id: int):
    ctx = get_user_context(user_id)
    ctx["last_activity"] = datetime.now()

def save_question_for_answer(user_id: int, ans_idx: int, question: str):
    ctx = get_user_context(user_id)
    ctx["question_index_map"][ans_idx] = question

def get_question_for_answer(user_id: int, ans_idx: int) -> str:
    ctx = get_user_context(user_id)
    return ctx.get("question_index_map", {}).get(ans_idx, "???")

def save_message_to_history(user_id: int, message: str, is_user: bool = True):
    ctx = get_user_context(user_id)
    prefix = "User: " if is_user else "Bot: "
    ctx["history"].append(f"{prefix}{message}")

def get_contextual_question(user_id: int, current_question: str) -> str:
    ctx = get_user_context(user_id)
    history = ctx.get("history", [])
    
    if not history:
        return current_question
    
    history_list = list(history)
    context_markers = ['–∞', '–∞ –µ—Å—Ç—å', '–∞ –∫–∞–∫', '–∞ —Å–∫–æ–ª—å–∫–æ', '–∞ —Å–∫–∏–¥–∫–∏', '–∞ —Ä–∞—Å—Å—Ä–æ—á–∫–∞', '–∞ –¥–æ–∫—É–º–µ–Ω—Ç', 
                       '–∏', '—Ç–æ–∂–µ', '—Ç–∞–∫–∂–µ', '–µ—â–µ', '–µ—â—ë', '–ø—Ä–æ–¥–æ–ª–∂–∏', '–¥–∞–ª–µ–µ']
    q_lower = current_question.lower()
    
    if len(q_lower) < 20 or any(marker in q_lower for marker in context_markers):
        recent_history = history_list[-3:] if len(history_list) >= 3 else history_list
        history_context = " ".join(recent_history)
        return f"{history_context} {current_question}"
    
    return current_question

def cleanup_inactive_users():
    now = datetime.now()
    to_delete = [
        uid for uid, ctx in user_contexts.items()
        if now - ctx.get("last_activity", now) > timedelta(hours=SETTINGS['inactivity_hours'])
    ]
    for uid in to_delete:
        del user_contexts[uid]

# ============================================================
# –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –°–°–´–õ–û–ö –î–õ–Ø –ö–ù–û–ü–û–ö
# ============================================================
def extract_links_and_buttons(text: str) -> Tuple[str, List[List[dict]]]:
    buttons = []
    url_pattern = r'(https?://[^\s<]+)'
    urls = re.findall(url_pattern, text)
    
    if urls:
        for raw_url in set(urls):
            clean_url = raw_url.replace("[add_button]", "").strip('.,;:!?() "\'[]{}')
            if not clean_url:
                continue
            
            label = "üîó –°—Å—ã–ª–∫–∞"
            if "roadmap" in clean_url.lower():
                label = "üó∫ –î–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞"
            elif "Business-card" in clean_url:
                label = "üë§ –û –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ"
            elif "calendar" in clean_url.lower():
                label = "üìÖ –í—ã–±—Ä–∞—Ç—å –≤—Ä–µ–º—è"
            
            buttons.append([{"text": label, "url": clean_url}])
        
        clean_text = re.sub(url_pattern, '', text).strip()
        return clean_text, buttons
    
    return text, []